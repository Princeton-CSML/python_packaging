{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66e49727-c8ad-47d5-ab0b-5c555b5661c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import ipytest\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72cafbc-76b8-4197-8f89-87278bb05d3e",
   "metadata": {},
   "source": [
    "## Why write tests\n",
    "1. save hours detecting bugs or inconsistencies, esp. if software continually developed\n",
    "2. smoother deployment, happier users, more citations?\n",
    "3. helps document your software by helping define its intent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2a241e-de01-4891-8b3b-657a84d16e6e",
   "metadata": {},
   "source": [
    "## What are tests: \n",
    "\n",
    "### Some *general* rules\n",
    "1. A test should ideally focus on one tiny bit of functionality\n",
    "2. Each test should be independent, capable of being run alone; i.e. each test gets a fresh dataset\n",
    "3. Tests should ideally run fast\n",
    "4. Run tests before and/or after a coding session to ensure nothing was broken, and certainly before pushing to a shared github repository\n",
    "\n",
    "### Kinds of tests relevant for academic software\n",
    "- Unit testing: checks that small, individual parts of code work properly\n",
    "- End-to-end testing: tests the full workflow\n",
    "- Functional testing: checks that program is behaving as expected\n",
    "    - does variables at any stage have unexpected/unrealistic values?\n",
    "- Performance testing: check speed, scalability, resource usage\n",
    "\n",
    "\n",
    "**code coverage**: fraction of source code executed during testing, higher is better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdffa215-0d6b-493a-b98a-311b7b715814",
   "metadata": {},
   "source": [
    "## How to write tests\n",
    "\n",
    "### [Pytest!](https://docs.pytest.org/en/6.2.x/contents.html)\n",
    "\n",
    "pytest is popular and easier to use.\n",
    "\n",
    "Typing `pytest` on command line searches for all files of the form test_\\*.py or \\*_test.py in current dir or subdirs and runs functions prefixed with \"test\" (but see [here](https://docs.pytest.org/en/6.2.x/goodpractices.html#test-discovery) for more info on test discovery).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b0a25b1-c68b-4028-8a07-2a00d891429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function.py\n",
    "def binom_coeff(x):\n",
    "    return x*(x-1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "02f864ff-b9ee-4a37-93bf-21fdc80f6d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "# test_function.py\n",
    "\n",
    "def test_binom_coeff():\n",
    "    assert binom_coeff(0) == 0\n",
    "    assert binom_coeff(1) == 0\n",
    "    assert binom_coeff(2) == 1\n",
    "    assert binom_coeff(6) == 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9d5b8c-05dc-48b6-aacc-73d7fcc80cc5",
   "metadata": {},
   "source": [
    "### pytest parameterization\n",
    "\n",
    "Combine multiple tests into one! Give test function multiple sets of arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee87bbac-d3c9-4577-a28e-a17f1d272ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                         [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "# test_function.py\n",
    "\n",
    "@pytest.mark.parametrize('input,expected', [\n",
    "    (0, 0),\n",
    "    (1, 0),\n",
    "    (2, 1),\n",
    "    (6, 15),\n",
    "])\n",
    "\n",
    "def test_binom_coeff(input, expected):\n",
    "    assert binom_coeff(input) == expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47fa4a1-8117-4a09-8387-d1db802ea1a9",
   "metadata": {},
   "source": [
    "### pytest fixtures\n",
    "\n",
    "These help set up a resource(s) before running a test. For instance, creating objects or staging files (later). We will use a trivial example here to first show the syntax. With complex functions to test, fixtures can do much more.\n",
    "\n",
    "Tell pytest that a function is a fixture by decorating it with `@pytest.fixture`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "458de245-73dd-4107-b5a6-1cdc6220eb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "# test_function.py\n",
    "\n",
    "@pytest.fixture\n",
    "def my_fixture():\n",
    "    # in practice with more complex functions, fixtures do more\n",
    "    return 6\n",
    "    \n",
    "def test_binom_coeff(my_fixture):\n",
    "    assert binom_coeff(my_fixture) == 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7e7c1a2-89dc-4caf-8231-d6e151df7d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "# test_function.py\n",
    "\n",
    "@pytest.fixture\n",
    "def my_fixture1():\n",
    "    return 6\n",
    "\n",
    "@pytest.fixture\n",
    "def my_fixture2():\n",
    "    return 15    \n",
    "    \n",
    "def test_binom_coeff(my_fixture1, my_fixture2):\n",
    "    assert binom_coeff(my_fixture1) == my_fixture2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6829b32-9916-4551-98b5-aed6c38f9480",
   "metadata": {},
   "source": [
    "### A real example\n",
    "\n",
    "Here is an example of an end-to-end test of a program called \"HATCHet\" from the Raphael lab. This program takes DNA sequences (in BAM files) from a patient's normal tissue and compares this to tumor samples to detect regions of the genome that have been duplicated or deleted.\n",
    "\n",
    "HATCHet takes in these BAM files, has multiple intermediate steps with output, and ultimately outputs a data table as a final result.\n",
    "\n",
    "In the example below, we \n",
    "- use pytest **fixtures** to store the locations of the input BAM files an also the locations of subdirectories in which intermediate results are stored \n",
    "- use pytest **mark** to skip the end-to-end test if a condition isn't satisfied (since this test takes some time)\n",
    "- compare the output file of the test function to a file previously produced by a working version of the program.\n",
    "\n",
    "While we provide the raw code below, we only want to show the general structure of the test using annotations that follow #'s. You can use this structure to write your own tests!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "603adff2-ecba-4070-85ed-1ba814129d8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mock'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pk/7tq3whvn1_1_1djt_750ykgw0000gp/T/ipykernel_97177/2010533099.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmock\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mock'"
     ]
    }
   ],
   "source": [
    "# test_script.py\n",
    "import pytest\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "from io import StringIO\n",
    "from mock import patch\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from pandas.testing import assert_frame_equal\n",
    "\n",
    "import hatchet\n",
    "from hatchet import config\n",
    "from hatchet.utils.count_reads import main as count_reads\n",
    "from hatchet.utils.genotype_snps import main as genotype_snps\n",
    "from hatchet.utils.count_alleles import main as count_alleles\n",
    "from hatchet.utils.combine_counts import main as combine_counts\n",
    "from hatchet.utils.cluster_bins import main as cluster_bins\n",
    "from hatchet.utils.plot_bins import main as plot_bins\n",
    "from hatchet.bin.HATCHet import main as main\n",
    "from hatchet.utils.plot_cn import main as plot_cn\n",
    "from hatchet.utils.solve import solver_available\n",
    "\n",
    "this_dir = os.path.dirname(__file__)\n",
    "SOLVE = os.path.join(os.path.dirname(hatchet.__file__), 'solve')\n",
    "\n",
    "#####\n",
    "# create 2 objects, normal_bam and tumor_bams, \n",
    "# that store the file location of the inputs: \n",
    "# the normal tissue BAM and all tumor tissue BAMs\n",
    "#####\n",
    "@pytest.fixture(scope='module')\n",
    "def bams():\n",
    "    bam_directory = config.tests.bam_directory\n",
    "    normal_bam = os.path.join(bam_directory, 'normal.bam')\n",
    "    if not os.path.exists(normal_bam):\n",
    "        pytest.skip('File not found: {}/{}'.format(bam_directory, normal_bam))\n",
    "    tumor_bams = sorted([f for f in glob.glob(bam_directory + '/*.bam') if os.path.basename(f) != 'normal.bam'])\n",
    "    if not tumor_bams:\n",
    "        pytest.skip('No tumor bams found in {}'.format(bam_directory))\n",
    "\n",
    "    return normal_bam, tumor_bams\n",
    "\n",
    "#####\n",
    "# create subdirectories to contain the output of all the intermediate steps\n",
    "#####\n",
    "@pytest.fixture(scope='module')\n",
    "def output_folder():\n",
    "    out = os.path.join(this_dir, 'out')\n",
    "    shutil.rmtree(out, ignore_errors=True)\n",
    "    for sub_folder in ('bin', 'snps', 'baf', 'bb', 'bbc', 'plots', 'results', 'evaluation', 'analysis'):\n",
    "        os.makedirs(os.path.join(out, sub_folder))\n",
    "    return out\n",
    "\n",
    "#####\n",
    "# the end-to-end test, which is skipped if a variable is not defined\n",
    "#####\n",
    "@pytest.mark.skipif(not config.paths.reference, reason='paths.reference not set')\n",
    "@patch('hatchet.utils.ArgParsing.extractChromosomes', return_value=['chr22'])\n",
    "def test_script(_, bams, output_folder):\n",
    "    normal_bam, tumor_bams = bams\n",
    "\n",
    "    count_reads(\n",
    "        args=[\n",
    "            '-N', normal_bam,\n",
    "            '-T'\n",
    "        ] + tumor_bams + [\n",
    "            '-b', '50kb',\n",
    "            '-st', config.paths.samtools,\n",
    "            '-S', 'Normal', 'Tumor1', 'Tumor2', 'Tumor3',\n",
    "            '-g', config.paths.reference,\n",
    "            '-j', '12',\n",
    "            '-q', '11',\n",
    "            '-O', os.path.join(output_folder, 'bin/normal.1bed'),\n",
    "            '-o', os.path.join(output_folder, 'bin/bulk.1bed'),\n",
    "            '-v'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    genotype_snps(\n",
    "        args=[\n",
    "            '-N', normal_bam,\n",
    "            '-r', config.paths.reference,\n",
    "            '-c', '290',    # min reads\n",
    "            '-C', '300',  # max reads\n",
    "            '-R', '',\n",
    "            '-o', os.path.join(output_folder, 'snps'),\n",
    "            '-st', config.paths.samtools,\n",
    "            '-bt', config.paths.bcftools,\n",
    "            '-j', '1'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    count_alleles(\n",
    "        args=[\n",
    "            '-bt', config.paths.bcftools,\n",
    "            '-st', config.paths.samtools,\n",
    "            '-N', normal_bam,\n",
    "            '-T'\n",
    "        ] + tumor_bams + [\n",
    "            '-S', 'Normal', 'Tumor1', 'Tumor2', 'Tumor3',\n",
    "            '-r', config.paths.reference,\n",
    "            '-j', '12',\n",
    "            '-q', '11',\n",
    "            '-Q', '11',\n",
    "            '-U', '11',\n",
    "            '-c', '8',\n",
    "            '-C', '300',\n",
    "            '-O', os.path.join(output_folder, 'baf/normal.1bed'),\n",
    "            '-o', os.path.join(output_folder, 'baf/bulk.1bed'),\n",
    "            '-L', os.path.join(output_folder, 'snps', 'chr22.vcf.gz'),\n",
    "            '-v'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    _stdout = sys.stdout\n",
    "    sys.stdout = StringIO()\n",
    "\n",
    "    combine_counts(args=[\n",
    "        '-c', os.path.join(output_folder, 'bin/normal.1bed'),\n",
    "        '-C', os.path.join(output_folder, 'bin/bulk.1bed'),\n",
    "        '-B', os.path.join(output_folder, 'baf/bulk.1bed'),\n",
    "        '-e', '12'\n",
    "    ])\n",
    "\n",
    "    out = sys.stdout.getvalue()\n",
    "    sys.stdout.close()\n",
    "    sys.stdout = _stdout\n",
    "\n",
    "    with open(os.path.join(output_folder, 'bb/bulk.bb'), 'w') as f:\n",
    "        f.write(out)\n",
    "\n",
    "    cluster_bins(args=[\n",
    "        os.path.join(output_folder, 'bb/bulk.bb'),\n",
    "        '-o', os.path.join(output_folder, 'bbc/bulk.seg'),\n",
    "        '-O', os.path.join(output_folder, 'bbc/bulk.bbc'),\n",
    "        '-e', '22171',  # random seed\n",
    "        '-tB', '0.04',\n",
    "        '-tR', '0.15',\n",
    "        '-d', '0.4'\n",
    "    ])\n",
    "\n",
    "    df1 = pd.read_csv(os.path.join(output_folder, 'bbc/bulk.seg'), sep='\\t')\n",
    "    df2 = pd.read_csv(os.path.join(this_dir, 'data', 'bulk.seg'), sep='\\t')\n",
    "    assert_frame_equal(df1, df2)\n",
    "\n",
    "    plot_bins(args=[\n",
    "        os.path.join(output_folder, 'bbc/bulk.bbc'),\n",
    "        '--rundir', os.path.join(output_folder, 'plots')\n",
    "    ])\n",
    "\n",
    "    if solver_available():\n",
    "        main(args=[\n",
    "            SOLVE,\n",
    "            '-x', os.path.join(output_folder, 'results'),\n",
    "            '-i', os.path.join(output_folder, 'bbc/bulk'),\n",
    "            '-n2',\n",
    "            '-p', '400',\n",
    "            '-v', '3',\n",
    "            '-u', '0.03',\n",
    "            '-r', '6700',  # random seed\n",
    "            '-j', '8',\n",
    "            '-eD', '6',\n",
    "            '-eT', '12',\n",
    "            '-g', '0.35',\n",
    "            '-l', '0.6'\n",
    "        ])\n",
    "\n",
    "        #####\n",
    "        # compare the final results of this test to those \n",
    "        # previously generated using the same input\n",
    "        # using assert_frame_equal\n",
    "        #####\n",
    "        df1 = pd.read_csv(os.path.join(output_folder, 'results/best.bbc.ucn'), sep='\\t')\n",
    "        df2 = pd.read_csv(os.path.join(this_dir, 'data', 'best.bbc.ucn'), sep='\\t')\n",
    "        assert_frame_equal(df1, df2)\n",
    "\n",
    "        plot_cn(args=[\n",
    "            os.path.join(output_folder, 'results/best.bbc.ucn'),\n",
    "            '--rundir', os.path.join(output_folder, 'evaluation')\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1ad176-c87b-4728-8501-93d227050d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterlab",
   "language": "python",
   "name": "jupyterlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
